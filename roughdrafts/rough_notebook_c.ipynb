{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a222ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import nltk.sentiment\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from acquire_c import *\n",
    "from prepare_c import *\n",
    "from explore_c import *\n",
    "\n",
    "plt.rc('figure', figsize=(13, 7))\n",
    "plt.style.use('seaborn-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5f4e02",
   "metadata": {},
   "source": [
    "## Acquire data and find the dominant language in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebd99db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not find the file javascript_clean_readme_75_z0.csv\n",
      "cleaning data, hold your horses....\n",
      "cleaning the orginial data\n",
      "Removing words who's zscore falls below the cutoff, this will take a moment\n",
      "calculating word counts, please wait...\n",
      "Before: 59827 words in the dataframe\n",
      "After: 5892 words will remain\n",
      "Removing the words from the column\n",
      "stemming the reduced cleaned data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesatchison/codeup-data-science/NLP-Project/prepare_c.py:180: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['stemmed'] = df['cleaned'].apply(stem)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatizing the reduced claned data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesatchison/codeup-data-science/NLP-Project/prepare_c.py:185: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['lemmatized'] = df['cleaned'].apply(lemmatize)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing words who's zscore falls below the cutoff, this will take a moment\n",
      "calculating word counts, please wait...\n",
      "Before: 182452 words in the dataframe\n",
      "After: 13954 words will remain\n",
      "Removing the words from the column\n"
     ]
    }
   ],
   "source": [
    "# You can pass a threshold argument but the default is 75\n",
    "lang = 'javascript'\n",
    "not_lang = f'not_{lang}'\n",
    "df = get_readme_data(lang=lang, lang_threshold=75, z_cutoff=0)\n",
    "df.prog_lang.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78ce517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data \n",
    "train, test = split(df)\n",
    "# Send the train data\n",
    "java_obj = NLP_explore(train, 'label', 'cleaned', lang, not_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eab665",
   "metadata": {},
   "source": [
    "## Explore\n",
    "\n",
    "### Look at word freqencies for JavaScript\n",
    "|          |   word_count |\n",
    "|:---------|-------------:|\n",
    "| data     |        25128 |\n",
    "| use      |        20312 |\n",
    "| gt       |        19874 |\n",
    "| yes      |        19795 |\n",
    "| code     |        18020 |\n",
    "| python   |        17961 |\n",
    "| using    |        17762 |\n",
    "| top      |        16057 |\n",
    "| project  |        15087 |\n",
    "| 1        |        13589 |\n",
    "| run      |        13366 |\n",
    "| api      |        12797 |\n",
    "| unknown  |        12742 |\n",
    "| github   |        12660 |\n",
    "| file     |        12109 |\n",
    "| learning |        11736 |\n",
    "| open     |        11354 |\n",
    "| app      |        11260 |\n",
    "| create   |        10836 |\n",
    "| 2        |        10439 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4495fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pd.DataFrame({'word_count': java_obj.all_freq}).head(20).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dbbbe7",
   "metadata": {},
   "source": [
    "## Look at some word count visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99db6e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at just JavaScript hplot\n",
    "java_obj.hplot_word_freq_viz(n=5, sort=lang)\n",
    "# Looking at just JavaScript bplot stacked\n",
    "java_obj.stacked_bplot_freq(n=5, sort=not_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a558e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at just not_JavaScript hplot\n",
    "java_obj.hplot_word_freq_viz(n=5, sort=lang)\n",
    "# Looking at just not_JavaScript bplot stacked\n",
    "java_obj.stacked_bplot_freq(n=5, sort=not_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc91068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at just all hplot\n",
    "java_obj.hplot_word_freq_viz(n=5)\n",
    "# Looking at just all bplot stacked\n",
    "java_obj.stacked_bplot_freq(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6729036f",
   "metadata": {},
   "source": [
    "## Look at N-Grams Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b8fe0",
   "metadata": {},
   "source": [
    "### Look at Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756b70b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_bigram = java_obj.n_gram(top_n= 10, col=lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9028bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "not_java_bigram = java_obj.n_gram(top_n = 10, col=not_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643bac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "both_bigrams = java_obj.n_gram(top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bb9b76",
   "metadata": {},
   "source": [
    "### Look at trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d07b52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "java_trigram = java_obj.n_gram(n=3, top_n=10, col=lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7b18fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_java_trigram = java_obj.n_gram(n=3, top_n=10, col=not_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd76969",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_trigram = java_obj.n_gram(n=3, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbafb181",
   "metadata": {},
   "source": [
    "### Plot some wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8b3df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_obj.plot_wordcloud(col=lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ff0b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_obj.plot_wordcloud(col=not_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a456d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_obj.plot_wordcloud()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4714bb9e",
   "metadata": {},
   "source": [
    "## Add some sentiment analysis and some features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308675fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sentiment analysis\n",
    "java_obj.add_sentiment_analysis()\n",
    "# Add features\n",
    "java_obj.add_features()\n",
    "\n",
    "java_obj.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82605f1",
   "metadata": {},
   "source": [
    "## Sentiment analysis bivariate plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc5cc6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "java_obj.sentiment_bivariate_plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f775d90",
   "metadata": {},
   "source": [
    "## Sentiment distribution plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a733b568",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_obj.sentiment_distributions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2ae05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
