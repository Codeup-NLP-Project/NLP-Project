{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a222ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import nltk.sentiment\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from acquire_c import *\n",
    "from prepare_c import *\n",
    "from explore_c import *\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "plt.rc('figure', figsize=(13, 7))\n",
    "plt.style.use('seaborn-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5f4e02",
   "metadata": {},
   "source": [
    "## Acquire data and find the dominant language in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ebd99db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You can pass a threshold argument but the default is 75\n",
    "df = get_readme_data()\n",
    "ongoing_stopwords = ['1', '2', '3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8190ea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_or_not(s:str, lang:str) -> str:\n",
    "    '''Takes string and returns if it is java or not java\n",
    "    '''\n",
    "    if s.lower() == lang:\n",
    "        return lang\n",
    "    return f'not_{lang}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78ce517",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'javascript'\n",
    "not_lang = f'not_{lang}'\n",
    "df['label']  = df.prog_lang.apply(lambda x: lang_or_not(x, lang))\n",
    "java_obj = NLP_explore(df, 'label', 'cleaned', lang, not_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eab665",
   "metadata": {},
   "source": [
    "## Explore\n",
    "\n",
    "### Look at word freqencies for JavaScript\n",
    "|          |   word_count |\n",
    "|:---------|-------------:|\n",
    "| data     |        25128 |\n",
    "| use      |        20312 |\n",
    "| gt       |        19874 |\n",
    "| yes      |        19795 |\n",
    "| code     |        18020 |\n",
    "| python   |        17961 |\n",
    "| using    |        17762 |\n",
    "| top      |        16057 |\n",
    "| project  |        15087 |\n",
    "| 1        |        13589 |\n",
    "| run      |        13366 |\n",
    "| api      |        12797 |\n",
    "| unknown  |        12742 |\n",
    "| github   |        12660 |\n",
    "| file     |        12109 |\n",
    "| learning |        11736 |\n",
    "| open     |        11354 |\n",
    "| app      |        11260 |\n",
    "| create   |        10836 |\n",
    "| 2        |        10439 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4495fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pd.DataFrame({'word_count': java_obj.all_freq}).head(20).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dbbbe7",
   "metadata": {},
   "source": [
    "## Look at some word count visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99db6e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at just JavaScript hplot\n",
    "java_obj.hplot_word_freq_viz(n=5, sort=lang)\n",
    "# Looking at just JavaScript bplot stacked\n",
    "java_obj.stacked_bplot_freq(n=5, sort=not_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a558e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at just not_JavaScript hplot\n",
    "java_obj.hplot_word_freq_viz(n=5, sort=lang)\n",
    "# Looking at just not_JavaScript bplot stacked\n",
    "java_obj.stacked_bplot_freq(n=5, sort=not_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc91068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at just all hplot\n",
    "java_obj.hplot_word_freq_viz(n=5)\n",
    "# Looking at just all bplot stacked\n",
    "java_obj.stacked_bplot_freq(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6729036f",
   "metadata": {},
   "source": [
    "## Look at N-Grams Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b8fe0",
   "metadata": {},
   "source": [
    "### Look at Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756b70b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_bigram = java_obj.n_gram(top_n= 10, col=lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9028bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_java_bigram = java_obj.n_gram(top_n = 10, col=not_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643bac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "both_bigrams = java_obj.n_gram(top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bb9b76",
   "metadata": {},
   "source": [
    "### Look at trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d07b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_trigram = java_obj.n_gram(n=3, top_n=10, col=lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7b18fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_java_trigram = java_obj.n_gram(n=3, top_n=10, col=not_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd76969",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trigram = java_obj.n_gram(n=3, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbafb181",
   "metadata": {},
   "source": [
    "### Plot some wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8b3df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_obj.plot_wordcloud(col=lang, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ff0b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_obj.plot_wordcloud(col=not_lang, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a456d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_obj.plot_wordcloud(save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4714bb9e",
   "metadata": {},
   "source": [
    "## Add some sentiment analysis and some features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308675fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sentiment analysis\n",
    "java_obj.add_sentiment_analysis()\n",
    "# Add features\n",
    "java_obj.add_features()\n",
    "\n",
    "java_obj.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82605f1",
   "metadata": {},
   "source": [
    "## Sentiment analysis bivariate plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc5cc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_obj.sentiment_bivariate_plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f775d90",
   "metadata": {},
   "source": [
    "## Sentiment distribution plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a733b568",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_obj.sentiment_distributions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2ae05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6a59ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9469aec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04aa537e",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "69b32290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_validate\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "66237290",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLP_model():\n",
    "    ''' Creates classification models using a variety of Sklearn models.\n",
    "\n",
    "        Models:\n",
    "        ----------------------------------------------------------------\n",
    "        KNeighborsClassifier, DecisionTreeClassifier, svm, GaussianNB, \n",
    "        MultinomialNB, GaussianProcessClassifier, MLPClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "        ----------------------------------------------------------------\n",
    "        \n",
    "        Arguments:\n",
    "            - data: Pandas DataFrame\n",
    "            - classifiers: List of classification models\n",
    "            - names: Names of classification models\n",
    "            - lang: Specifies a language to create a lang/not_lang label from\n",
    "            - top_langs: Specifies the top n langs to create labels for, non-top_langs will be labeled 'other'\n",
    "    '''\n",
    "    def __init__(self, data:pd.DataFrame, classifiers: list, names: list, lang = None, top_langs = None):\n",
    "        ''' Passes dataframe, list of actual classifiers and their names, as well as checks \n",
    "            for kwargs lang or top_lang\n",
    "            Creates a zip of classifiers and their names\n",
    "        '''\n",
    "        \n",
    "        df = data.copy(deep = True)\n",
    "        \n",
    "        #Checking for individual language specified or n_langs and creating label column\n",
    "        # For individual lang specification\n",
    "        if lang and not top_langs: # Checking for lang\n",
    "            self.lang = lang # assigning lang attribute\n",
    "            # creating label column\n",
    "            df['label'] = df.prog_lang.apply(lambda x: x.lower() if x == self.lang else f'not_{self.lang.lower()}')\n",
    "        if top_langs and not langs: # Checking for top_langs\n",
    "            self.top_langs = df.prog_lang.value_counts()[:n_langs] # getting top n langs\n",
    "            # Creating labels column from top n languages            \n",
    "            df['label'] = df.prog_lang.apply(lambda x: x.lower() if x in self.top_langs else 'other')\n",
    "        if lang and top_langs:\n",
    "            raise AttributeError('Must specify either lang or top_langs, cant create labels for both')\n",
    "        \n",
    "        # Clean dataframe\n",
    "        df.lemmatized = df.lemmatized.apply(basic_clean)\n",
    "        \n",
    "        # Creating class attributes\n",
    "        self.classifiers = classifiers.copy()\n",
    "        self.names = names.copy()\n",
    "        \n",
    "        # Creating class instance of df\n",
    "        self.df = df.copy(deep = True)\n",
    "        \n",
    "        self.models = zip(self.names, self.classifiers) # zipping models and names\n",
    "        \n",
    "        \n",
    "    def split(self, target = None):\n",
    "        '''\n",
    "        This function takes in a dataframe and, optionally, a target_var array. Performs a train, validate, \n",
    "        test split with no stratification. Returns train, validate, and test dfs.\n",
    "        '''\n",
    "        \n",
    "        # Checking for y specified\n",
    "        if target is None: # if no y, preform regular train, validate, test split\n",
    "            train_validate, test = train_test_split(self.df, test_size=.2, \n",
    "                                                    random_state=1312)\n",
    "            train, validate = train_test_split(train_validate, test_size=.3, \n",
    "                                                    random_state=1312)\n",
    "            \n",
    "            self.train, self.validate, self.test = train, validate, test # setting self versions of each df\n",
    "            return train, validate, test\n",
    "        \n",
    "        # If y is specified preform X/y train, validate, test split\n",
    "        else:\n",
    "            X_train_validate, X_test, y_train_validate, y_test = train_test_split(self.df, target,\n",
    "                                                                            test_size=.2, \n",
    "                                                                            random_state=1312)\n",
    "            X_train, X_validate, y_train, y_validate = train_test_split(X_train_validate, y_train_validate,\n",
    "                                                                            test_size=.3, \n",
    "                                                                            random_state=1312)\n",
    "            self.X_train, self.X_validate, self.X_test,\\\n",
    "            self.y_train, self.y_validate, self.y_test = X_train, X_validate, X_test, y_train,\\\n",
    "                                                        y_validate, y_test # attributes for each X/y df and array\n",
    "            \n",
    "            return X_train, X_validate, X_test, y_train, y_validate, y_test\n",
    "    \n",
    "    \n",
    "    def tf(self):\n",
    "        ''' Gets the term frequency of lematized column in the df\n",
    "        '''\n",
    "        \n",
    "        # For each lemmatized doc, append to series\n",
    "        docs = [] # init empty series for split documents\n",
    "        words = [] # init empty series for unique words\n",
    "        for doc in self.df['lemmatized'].values:\n",
    "            for word in doc.split(): # iterating through each word in a split doc\n",
    "                words.append(word) # add to words\n",
    "        \n",
    "        word_ser = pd.Series(words) # turn w\n",
    "        \n",
    "        # Creating a df from unique words containing raw term count, \n",
    "        tf_df = (pd.DataFrame({'raw_count': word_ser.value_counts()})) # raw counts of each term\n",
    "        tf_df['frequency'] = tf_df.raw_count / tf_df.raw_count.sum() # frequency of each term\n",
    "        tf_df['augmented_frequency'] = tf_df.frequency / tf_df.frequency.max() # augmented freq of words\n",
    "        \n",
    "        return tf_df\n",
    "    \n",
    "    \n",
    "    def tf_idf(self):\n",
    "        return 'Yet to make method'\n",
    "    \n",
    "        \n",
    "    def metrics(self, metric_type = 'accuracy', splits = 3):\n",
    "        ''' Creates a metrics df measuring metric_type, accuracy by default.\n",
    "            Preforms a kfold a number of times determined by splits\n",
    "        '''\n",
    "        try: # checking if label exists, if not, didnt specify a lang or top_langs\n",
    "            self.df['label']\n",
    "        except KeyError:\n",
    "            return KeyError('Must specify language target in class to create model')\n",
    "            \n",
    "        target = 'label' # Setting target to label\n",
    "        \n",
    "        result = [] # init empty results list\n",
    "        for i, (self.name, self.classifier) in enumerate(self.models): # iterate through zipped models\n",
    "            kfold = KFold(n_splits = splits) # number of kfolds set to splits\n",
    "            scores = cross_validate(self.classifier, self.df.drop(columns = [target]), self.df[target], cv = kfold, scoring = metric_type) # cross validate on each kfold\n",
    "            result.append(scores) # append to results\n",
    "            \n",
    "        results = [res['test_score'].mean() for res in result] # list comp to get mean of cross val tests for each model\n",
    "        metrics_df = pd.DataFrame(data = zip(self.names, results), columns = ['model', metric_type]) # wrap zipped model names and results in dataframe\n",
    "        return metrics_df.sort_values(by = [metric_type], ascending = False) # return sorted by metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "04ebb6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['K Nearest Neighbors', 'Decision Tree', 'Random Forest', 'AddaBoost', \n",
    "         'Gaussian N-Bayes', 'Multinomial N-Bayes']\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(n_neighbors = 3),\n",
    "    DecisionTreeClassifier(max_depth = 5),\n",
    "    RandomForestClassifier(max_depth = 5, n_estimators = 10, max_features = 1 ),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    MultinomialNB()\n",
    "    ]\n",
    "\n",
    "model_obj = NLP_model(df, classifiers, names, lang = 'Python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "d123a674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model_obj.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "11081b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_models(X_data, y_data, classifier_names, classifier_models):\n",
    "    '''\n",
    "        Takes two arrays:\n",
    "        - X_data = data without the target_var included\n",
    "        - y_data = an array of the target_var\n",
    "        - List of model names \n",
    "        - List of the classifiers themselves\n",
    "        \n",
    "        Preforms K-fold and cross-validation and returns a metrics dataframe with the model name and accuracy score. \n",
    "    '''\n",
    "    # Zipping models and Classifiers\n",
    "    models = zip(classifier_names, classifier_models)\n",
    "\n",
    "    # Init empty lists\n",
    "    names = [] \n",
    "    result = []\n",
    "    coeff = []\n",
    "\n",
    "    # Cross-validating accuracy for each model based on Train subset\n",
    "    for i, (name, model) in enumerate(models):\n",
    "        kfold = KFold(n_splits = 10)\n",
    "        scores = cross_validate(model, X_data, y_data, cv = kfold, scoring = 'accuracy', return_estimator=True)\n",
    "        result.append(scores)\n",
    "        names.append(name)\n",
    "        try:\n",
    "            coeff.append(model.coeff_)\n",
    "        except AttributeError:\n",
    "            coeff.append(None)\n",
    "        msg = \"{0}: Accuracy: {1}, Coeff: {2}\".format(name, scores['test_score'].mean(), coeff[i])\n",
    "        print(msg)\n",
    "        \n",
    "    results = [res['test_score'].mean() for res in result]\n",
    "    metrics_df = pd.DataFrame(data = zip(names, results), columns = ['Model', 'Accuracy'])\n",
    "    return metrics_df.sort_values(by = ['Accuracy'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24760107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
