{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22ebbfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import nltk.sentiment\n",
    "\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "plt.rc('figure', figsize=(13, 7))\n",
    "plt.style.use('seaborn-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40104364",
   "metadata": {},
   "source": [
    "### This `fetch_user_followers_and_repos` function is to query a user's followers and the first 30 repositiories that they have publicly available on Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb5a7030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_user_followers_and_repos(user: str):\n",
    "    '''Takes a Github username as a string and will parse their github for followers\n",
    "    and the users first upto 30 publicly available repos and returns 2 lists the followers and repositories\n",
    "    '''\n",
    "    # Set headers\n",
    "    headers= {'User-Agent': 'Codeup Data Science'}\n",
    "    # Set the parse_followers flag to True\n",
    "    parse_followers = True\n",
    "    # Start with page 1\n",
    "    page = 1\n",
    "    # Build a list to hold all the parsed followers\n",
    "    all_users_followers = list()\n",
    "    \n",
    "    # Run through all pages of followers until there are not any more followers\n",
    "    while parse_followers:\n",
    "        # Runs to pull all followers from the user\n",
    "        # Builds follower url to find the users follower's \n",
    "        follower_url = f'https://github.com/{user}?page={page}&tab=followers'\n",
    "        print(follower_url)\n",
    "        # Fetch response from url\n",
    "        response = get(follower_url, headers=headers)\n",
    "\n",
    "        # Return the user's followers html page\n",
    "        follower_html = str(BeautifulSoup(response.text, 'html.parser'))\n",
    "\n",
    "        # This regex pulls to pull followers usernames out of the html page\n",
    "        # set as a set so it doesn't repeat users and remove the first char spot for the '/'\n",
    "        user_followers = set([r[1][1:] for r in re.findall(r'(link_type\\:self\"\\shref=\")(.*?)\"', follower_html)])\n",
    "        \n",
    "        # Add the new list of followers to the all_users_followers\n",
    "        all_users_followers.extend(user_followers)\n",
    "        \n",
    "        # Check to see if there are not any more followers\n",
    "        if not user_followers:\n",
    "            parse_followers = False\n",
    "        # Move onto the next page   \n",
    "        page += 1\n",
    "    \n",
    "    # Build users repos\n",
    "    repo_url = f'https://github.com/{user}?tab=repositories'\n",
    "    \n",
    "    # get the response from github repos\n",
    "    response = get(repo_url, headers=headers) \n",
    "    \n",
    "    # Return the user's repo html page\n",
    "    repo_str = str(BeautifulSoup(response.text, 'html.parser'))\n",
    "    \n",
    "    # Filter the first page of repos and put into a set\n",
    "    repos = set([r[1] for r in re.findall(\n",
    "        r'(itemprop=\"name\\scodeRepository\">\\n)\\s*(.*?)<', repo_str)])\n",
    "    \n",
    "    return all_users_followers, repos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b115b6cb",
   "metadata": {},
   "source": [
    "## Next we need to send the usernames to the function to pull all the information out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1dca33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_followers_to_dataframe(df, followers: list):\n",
    "    '''Takes a dataframe and a list of followers and checks if the followers are in the dataframe,\n",
    "    if they are not, they will be added and set their default parsed value to False, returns\n",
    "    amended dataframe\n",
    "    '''\n",
    "    # pull the followers from the dataframe and set to a list to search later\n",
    "    followers_in_dataframe = df.index.to_list()\n",
    "    \n",
    "    # Iterate through the followers\n",
    "    for follower in followers:\n",
    "        # Check if the follower is in the dataframe\n",
    "        if follower not in followers_in_dataframe:\n",
    "            # Set the default parsed value to False\n",
    "            df.loc[follower] = {'parsed': False}\n",
    "    # Return the amended dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67efa331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_repos_to_readme_dataframe(user:str, repos:list):\n",
    "    '''Takes the current user and the repos to search through and see if they are in the readme_dataframe index.\n",
    "    If the repo is not in the readme_dataframe, it will add it to the index to pull the readme out later.\n",
    "    '''\n",
    "    # Set the filename for the readme\n",
    "    filename = 'readme_data_c.csv'\n",
    "    \n",
    "    # Change the repos to a list so it can be scripted\n",
    "    repos = list(repos)\n",
    "    \n",
    "    # Define the default columns for the dataframe\n",
    "    default_cols ={\n",
    "            'parsed': False, # Default parsed is False\n",
    "            'readme': 'None', # Default readme is 'None'\n",
    "            'programming_language': 'None' # Default programming language is 'None'\n",
    "        }\n",
    "    \n",
    "    # Checks to see if the readme exists\n",
    "    if os.path.exists(filename):\n",
    "        # pull the readme_df in and set the index col to the user_repo\n",
    "        readme_df = pd.read_csv(filename, index_col='user_repo')\n",
    "    # If the readme_df does not exist, create it and set the default features\n",
    "    else:\n",
    "        # Add the user_repo to the default cols to set as index\n",
    "        default_cols['user_repo'] = user + '/' + repos[0]\n",
    "        # Define the dataframe\n",
    "        readme_df = pd.DataFrame([default_cols]).set_index('user_repo')\n",
    "        # Remove from default_cols so no key errors\n",
    "        default_cols.pop('user_repo')\n",
    "    \n",
    "    # Iterate though the repos and check if they are in the readme_dataframe or not\n",
    "    for repo in list(repos):\n",
    "        # Combine user with repo to define the target index\n",
    "        ind = user + '/' + repo\n",
    "        # If ind is not in the readme file add it to it with the default cols\n",
    "        if ind not in readme_df.index.to_list():\n",
    "            # Add the user_repo to the readme with default_cols\n",
    "            readme_df.loc[ind] = default_cols\n",
    "    # Save the readme_df \n",
    "    readme_df.to_csv(filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08a67a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_github(target: str, reparse=False, readme_cutoff=1000):\n",
    "    '''Specify user to start a crawl and like to have run looking for\n",
    "    user/repos, and README.md files\n",
    "    '''\n",
    "    # Establish users data that was crawled and if it was parsed or not\n",
    "    parsed_users_file = 'users_data_c.csv'\n",
    "    \n",
    "    # Try to fetch readme_data_c to see how many entries there are to check against cutoff\n",
    "    try:\n",
    "        # Check the readme length\n",
    "        readme_len = len(pd.read_csv('readme_data_c.csv', index_col=['user_repo']))\n",
    "        \n",
    "    except:\n",
    "        # if the file does not exist set the readme length to 0\n",
    "        readme_len = 0\n",
    "    \n",
    "    # Show the number of README Destinations\n",
    "    print('Current number of README destinations is:', readme_len)\n",
    "\n",
    "    # Check if there is a file for that site\n",
    "    if os.path.exists(parsed_users_file):\n",
    "        # Pull the dataframe in\n",
    "        df = pd.read_csv(parsed_users_file, index_col='user')\n",
    "        # check if the target is in the dataframe\n",
    "        if reparse:\n",
    "            df.loc[target, 'parsed'] = False\n",
    "        \n",
    "        # Check if user is in the list\n",
    "        elif target not in df.index.to_list():\n",
    "            # Add target to the dataframe\n",
    "            df.loc[target] = {'parsed': False}\n",
    "        # Ensure the number of readme files is greater than the cutoff\n",
    "        elif readme_len >= readme_cutoff:\n",
    "            # If greater than the cutoff return the dataframe\n",
    "            return df\n",
    "            \n",
    "    else:\n",
    "        # If the dataframe does not exist, set the first value to the target\n",
    "        # Set the user and set the user as the index\n",
    "        df = pd.DataFrame([{'user': target, 'parsed': False}]).set_index('user')\n",
    "            \n",
    "    # Ensure there are no more users to parse\n",
    "    while len(df[~df.parsed].parsed.to_list()) != 0 and readme_len < readme_cutoff:\n",
    "        # Pull the dataframe in again to ensure it's fresh each iteration\n",
    "        if os.path.exists(parsed_users_file):\n",
    "        # Pull the dataframe in if it's not the first time\n",
    "            if reparse:\n",
    "                # Ensure the user is set to not parsed if reparse is True\n",
    "                df.loc[target, 'parsed'] = False\n",
    "                # Set reparse flag to False\n",
    "                reparse = False\n",
    "                \n",
    "            # If not reparse and the user is not in the index\n",
    "            elif target not in df.index.to_list():\n",
    "                # Add user to the dataframe with parsed to False\n",
    "                df.loc[target] = {'parsed': False}\n",
    "            \n",
    "        # Set the user to parse as the first element in the list of NON parsed users\n",
    "        user = df[~df.parsed].index[0]\n",
    "\n",
    "        # Returns a list of followers and a list of repositories\n",
    "        followers, repos = fetch_user_followers_and_repos(user)\n",
    "        \n",
    "        # Have function check if any of the followers are in the dataframe already\n",
    "        df = add_followers_to_dataframe(df, followers)\n",
    "        \n",
    "        # Send current user and first 30 repos to be added to readme_df\n",
    "        add_repos_to_readme_dataframe(user, repos)\n",
    "        \n",
    "        # Get the number of readme entries and check against cutoff\n",
    "        readme_len = len(pd.read_csv('readme_data_c.csv', index_col=['user_repo']))\n",
    "        \n",
    "        print('Current number of README destinations is:', readme_len)\n",
    "        \n",
    "        # Set the current user parsed to True to iterate to next user\n",
    "        df.loc[user] = {'parsed': True}\n",
    "    \n",
    "        # Save the dataframe so that it can continue to go through and check each url\n",
    "        df.to_csv(parsed_users_file)\n",
    "        # Show that the user was successfully parsed\n",
    "        print(f'Parsed {user}')\n",
    "        \n",
    "    # Return dataframe of followers when done\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd66daee",
   "metadata": {},
   "source": [
    "### Run this to continue populating the readme_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6f78608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawl_github('abhisheknaiidu', readme_cutoff=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712a7074",
   "metadata": {},
   "source": [
    "## Now that we have the README destinations, we need to pull the README text and the associated script langauge that is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b64e8809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_and_lang(url):\n",
    "    ''' Takes a Github url repo and pulls out the text and the associated\n",
    "    programming language and percentage and returns a string of text and\n",
    "    and a dictonary of languages formatted as a string where structure is \n",
    "    like this \n",
    "    '[{'programming_language': '__some_language__',\n",
    "        'percentage': float(__some_percentage__)}]'\n",
    "    '''\n",
    "    # Set headers for BeautifulSoup request\n",
    "    headers= {'User-Agent': 'Codeup Data Science'}\n",
    "    \n",
    "    # Fetch response from url\n",
    "    response = get(url, headers=headers)\n",
    "    # Create Soup html object\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # arguments be passed to the soup find_all function for text\n",
    "    kwargs = {'class': 'markdown-body entry-content container-lg'}\n",
    "    # returns the text from the soup object\n",
    "    text_html = soup.find_all(**kwargs)\n",
    "    # Extracted text from the readme file\n",
    "    text_list = re.findall(r'>(.*?)<', str(text_html))\n",
    "    \n",
    "    # Filter out all blank strings and spaces and concat all together\n",
    "    text = ' '.join([txt for txt in text_list if txt not in ['', ' ']])\n",
    "    \n",
    "    # arguments be passed to the soup find_all function for language\n",
    "    kwargs = {\n",
    "        'data-ga-click': 'Repository, language stats search click, location:repo overview'\n",
    "    }\n",
    "    # Find the raw languange html from url\n",
    "    lang_html = soup.find_all(**kwargs)\n",
    "    # regex out all the text from the html and remove\n",
    "    # all the undesired results\n",
    "    l_list = [txt for txt in re.findall(\n",
    "        r'>(.*?)<', str(lang_html)) if txt not in ['', ' ', ', ']]\n",
    "    \n",
    "    # Iterate though the list and put the language and percentage into a \n",
    "    # list of dicts\n",
    "    languages = [{'programming_language': l_list[n],\n",
    "             'percentage': float(l_list[n+1][:-1])} for n in range(0, len(l_list), 2)]\n",
    "    \n",
    "    # Returns the text and the languages as strings\n",
    "    return text, str(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34230330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_readme_destinations(cutoff=10000, reparse=False, n=0):\n",
    "    '''Iterate though and parse README destinations untill we reach the cutoff limiter from the\n",
    "    readme_data_c file. The default cutoff is 1000 and reparse default to False and n is the\n",
    "    starting point for a reparse and will not work unless it's marked to True\n",
    "    '''\n",
    "    # Set the filename for the readme\n",
    "    filename = 'readme_data_c_copy.csv'\n",
    "    \n",
    "    # Checks to see if the readme exists\n",
    "    if os.path.exists(filename):\n",
    "        # pull the readme_df in and set the index col to the user_repo and\n",
    "        # Reset the index so that the user_repo won't have issues being pulled out\n",
    "        readme_df = pd.read_csv(filename, index_col='user_repo').reset_index()\n",
    "    \n",
    "    # If the readme_df does not exist print that it does not exist and run github_crawl first\n",
    "    else:\n",
    "        print(f'{filename} does not exist, run github_crawl first to build the file')\n",
    "        return None\n",
    "    \n",
    "    # If reparse is requested set the readme_len to 0 so it will iterates\n",
    "    if reparse:\n",
    "        readme_len = 0\n",
    "        \n",
    "    # Otherwise calculate the len of the readme file\n",
    "    else: \n",
    "        # Calculate the number of parsed destinations\n",
    "        readme_len = len(readme_df[readme_df.parsed])\n",
    "    \n",
    "    # Check how many destinations were successfully parsed and if \n",
    "    # it is greater or equal to the cutoff return the dataframe\n",
    "    if readme_len >= cutoff:\n",
    "        # Return readme dataframe\n",
    "        return readme_df\n",
    "    \n",
    "    # Continue parsing until the number of parsed values is greater than or equal to\n",
    "    # the defined cutoff\n",
    "    while readme_len < cutoff:\n",
    "        # pull the readme_df in and set the index col to the user_repo and\n",
    "        # Reset the index so that the user_repo won't have issues being pulled out\n",
    "        readme_df = pd.read_csv(filename, index_col='user_repo').reset_index()\n",
    "                \n",
    "        # If reparse is desired just select from \n",
    "        if reparse:\n",
    "            # Select the user_repo destination from readme_df and input into the url\n",
    "            # Select the current readme to update later\n",
    "            current_readme = readme_df.iloc[n]\n",
    "            # Fetch the user_repo string\n",
    "            user_repo = current_readme.user_repo\n",
    "            # Build the url to the repo\n",
    "            url = f'https://github.com/{user_repo}'\n",
    "            # Increase n to get the next one next time\n",
    "            n += 1\n",
    "        else:\n",
    "            # Select the first destination and build the URL target\n",
    "            # from the non-parsed values if reparse is not requested\n",
    "            # Select the user_repo destination from readme_df and input into the url\n",
    "            # Select current readme to update later\n",
    "            # Select the first index that is False\n",
    "            n = (readme_df.parsed == False).idxmax()\n",
    "            current_readme = readme_df.loc[n]\n",
    "            # Fetch the user_repo string\n",
    "            user_repo = current_readme.user_repo\n",
    "            # Build the url the repo\n",
    "            url = f'https://github.com/{current_readme.user_repo}'\n",
    "            \n",
    "        # Send the url to have the text and language be parsed and extracted\n",
    "        text, lang= extract_text_and_lang(url)\n",
    "        \n",
    "        # These are going to be the values that are entered into the \n",
    "        # dataframe\n",
    "        new_values = [\n",
    "            user_repo, # The user_repo name\n",
    "            True, # Set the value to True\n",
    "            text, # The text combined that was queried\n",
    "            lang # string of programming languages \n",
    "        ]\n",
    "        # Check if the there is not any text in the readme\n",
    "        if text == '':\n",
    "            readme_df = readme_df.drop(n)\n",
    "        # otherwise update with new values\n",
    "        else:\n",
    "            # Update the current readme index with the values\n",
    "            readme_df.loc[n] = new_values\n",
    "        # update the readme_len\n",
    "        readme_len = len(readme_df[readme_df.parsed])\n",
    "        print(readme_len)\n",
    "        \n",
    "        # Set the index to the user_repo\n",
    "        readme_df= readme_df.set_index('user_repo')\n",
    "        # Save the file so it keeps progress\n",
    "        readme_df.to_csv(filename)\n",
    "        \n",
    "    # Once cutoff is reached, return the readme_df\n",
    "    return readme_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083853e1",
   "metadata": {},
   "source": [
    "## Run below to get the README text and programming language updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc0a1bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "26\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "32\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "60\n",
      "61\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "75\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "80\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "84\n",
      "85\n",
      "85\n",
      "86\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n"
     ]
    }
   ],
   "source": [
    "times = 0\n",
    "while True:\n",
    "    try:\n",
    "        parse_readme_destinations(100000)\n",
    "    except:\n",
    "        if times == 20:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
